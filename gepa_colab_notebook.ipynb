{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEPA Refiner Prompt Optimization\n",
    "\n",
    "This notebook runs GEPA optimization on the contrastive verifier's refiner prompts using Google Colab with Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your contrastive_verifier directory\n",
    "# Modify this path to match your Google Drive folder structure\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/contrastive_verifier\"\n",
    "\n",
    "import os\n",
    "os.chdir(DRIVE_PATH)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Files: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup HuggingFace Token (for Gemma access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set token directly (replace with your token)\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"your_huggingface_token_here\"  # <-- REPLACE THIS\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Option 2: Or load from .env file if it exists\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import and Run GEPA Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add paths\n",
    "import sys\n",
    "sys.path.insert(0, DRIVE_PATH)\n",
    "sys.path.insert(0, os.path.join(DRIVE_PATH, 'gepa/src'))\n",
    "\n",
    "# Import from our modules\n",
    "from gepa_refiner_adapter import (\n",
    "    ContrastiveVerifierAdapter,\n",
    "    load_verifier,\n",
    "    load_refiner_model,\n",
    "    prepare_training_data,\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"max_iterations\": 25,\n",
    "    \"minibatch_size\": 10,\n",
    "    \"max_train_samples\": -1,  # -1 for all samples\n",
    "    \"score_threshold\": 0.7,\n",
    "    \"verifier_checkpoint\": \"verifier_best.pt\",\n",
    "    \"scored_file\": \"scored_outputs.jsonl\",\n",
    "    \"original_file\": \"generations_google-gemma-2b-it_0_-1.jsonl\",\n",
    "    \"output_file\": \"evolved_prompts.json\",\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "# Seed prompts (baseline)\n",
    "SEED_PROMPTS = {\n",
    "    \"critique_prompt\": \"\"\"Review this solution step by step. Identify any errors in mathematical reasoning, calculations, or logic. Be specific about what went wrong.\"\"\",\n",
    "    \"refinement_prompt\": \"\"\"Based on the critique above, provide a corrected step-by-step solution. End your final answer with \\\\boxed{answer}.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Configuration set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "print(\"=\"*60)\n",
    "print(\"1. Loading verifier model...\")\n",
    "verifier_model, verifier_tokenizer = load_verifier(\n",
    "    CONFIG[\"verifier_checkpoint\"], \n",
    "    CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "print(\"\\n2. Loading refiner model (also used for reflection)...\")\n",
    "refiner_model, refiner_tokenizer = load_refiner_model(\n",
    "    \"google/gemma-2-2b-it\", \n",
    "    CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "print(\"\\nModels loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"3. Preparing training data...\")\n",
    "trainset, valset = prepare_training_data(\n",
    "    scored_file=CONFIG[\"scored_file\"],\n",
    "    original_file=CONFIG[\"original_file\"],\n",
    "    max_samples=CONFIG[\"max_train_samples\"],\n",
    "    score_threshold=CONFIG[\"score_threshold\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create adapter\n",
    "print(\"4. Creating GEPA adapter...\")\n",
    "adapter = ContrastiveVerifierAdapter(\n",
    "    verifier_model=verifier_model,\n",
    "    verifier_tokenizer=verifier_tokenizer,\n",
    "    refiner_model=refiner_model,\n",
    "    refiner_tokenizer=refiner_tokenizer,\n",
    "    device=CONFIG[\"device\"]\n",
    ")\n",
    "print(\"Adapter created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the optimizer class from the script\n",
    "# We'll define it inline to avoid import issues\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Reflection prompt template\n",
    "REFLECTION_PROMPT_TEMPLATE = \"\"\"I provided an assistant with the following instructions to generate critiques and refinements for math solutions:\n",
    "\n",
    "```\n",
    "<curr_instructions>\n",
    "```\n",
    "\n",
    "The following are examples of different inputs provided to the assistant, along with its critique/refinement outputs and feedback on how well the refinement worked:\n",
    "\n",
    "```\n",
    "<inputs_outputs_feedback>\n",
    "```\n",
    "\n",
    "Your task is to write improved instructions for the assistant.\n",
    "\n",
    "Key observations from the examples:\n",
    "1. Look at cases where refinement SUCCEEDED - what made the critique effective?\n",
    "2. Look at cases where refinement FAILED - was the critique missing the actual error?\n",
    "3. Look at REGRESSIONS where correct solutions became incorrect - what went wrong?\n",
    "\n",
    "When writing new instructions:\n",
    "- Include specific strategies that worked in successful cases\n",
    "- Add warnings about common failure patterns observed\n",
    "- Include domain-specific math reasoning guidance\n",
    "- Be explicit about how to identify calculation vs. conceptual errors\n",
    "\n",
    "Provide the new instructions within ``` blocks.\n",
    "\"\"\"\n",
    "\n",
    "class SimpleGEPAOptimizer:\n",
    "    def __init__(self, adapter, trainset, valset, reflection_model, reflection_tokenizer, device=\"cuda\"):\n",
    "        self.adapter = adapter\n",
    "        self.trainset = trainset\n",
    "        self.valset = valset\n",
    "        self.reflection_model = reflection_model\n",
    "        self.reflection_tokenizer = reflection_tokenizer\n",
    "        self.device = device\n",
    "        self.candidates = []\n",
    "        self.scores = []\n",
    "        self.history = []\n",
    "    \n",
    "    def _evaluate_on_valset(self, candidate):\n",
    "        eval_result = self.adapter.evaluate(self.valset, candidate, capture_traces=False)\n",
    "        return sum(eval_result.scores) / len(eval_result.scores) if eval_result.scores else 0.0\n",
    "    \n",
    "    def _sample_minibatch(self, size=5):\n",
    "        return random.sample(self.trainset, min(size, len(self.trainset)))\n",
    "    \n",
    "    def _generate_reflection(self, current_prompt, reflective_dataset, component_name):\n",
    "        formatted_examples = []\n",
    "        for i, record in enumerate(reflective_dataset):\n",
    "            example = f\"\"\"### Example {i+1}\n",
    "Question: {record.get('Question', 'N/A')[:200]}...\n",
    "Original Solution (excerpt): {record.get('Original_Solution', 'N/A')[:200]}...\n",
    "Critique Generated: {record.get('Critique_Generated', 'N/A')[:300]}...\n",
    "Refined Solution (excerpt): {record.get('Refined_Solution', 'N/A')[:200]}...\n",
    "Feedback: {record.get('Feedback', 'N/A')}\n",
    "\"\"\"\n",
    "            formatted_examples.append(example)\n",
    "        \n",
    "        examples_text = \"\\n\\n\".join(formatted_examples)\n",
    "        reflection_prompt = REFLECTION_PROMPT_TEMPLATE.replace(\n",
    "            \"<curr_instructions>\", current_prompt\n",
    "        ).replace(\n",
    "            \"<inputs_outputs_feedback>\", examples_text\n",
    "        )\n",
    "        \n",
    "        full_prompt = f\"\"\"<start_of_turn>user\n",
    "{reflection_prompt}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "        \n",
    "        inputs = self.reflection_tokenizer(\n",
    "            full_prompt, return_tensors=\"pt\", truncation=True, max_length=4096\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.reflection_model.generate(\n",
    "                **inputs, max_new_tokens=1024, temperature=0.7, do_sample=True, top_p=0.9,\n",
    "                pad_token_id=self.reflection_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.reflection_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"<start_of_turn>model\" in response:\n",
    "            response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
    "        \n",
    "        match = re.search(r'```(?:\\w*\\n)?(.+?)```', response, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        if \"```\" in response:\n",
    "            parts = response.split(\"```\")\n",
    "            if len(parts) > 1:\n",
    "                return parts[1].strip()\n",
    "        return response.strip()\n",
    "    \n",
    "    def optimize(self, seed_candidate, max_iterations=10, minibatch_size=5, components_to_optimize=None):\n",
    "        if components_to_optimize is None:\n",
    "            components_to_optimize = [\"critique_prompt\", \"refinement_prompt\"]\n",
    "        \n",
    "        current_candidate = seed_candidate.copy()\n",
    "        self.candidates.append(current_candidate.copy())\n",
    "        \n",
    "        seed_score = self._evaluate_on_valset(current_candidate)\n",
    "        self.scores.append(seed_score)\n",
    "        print(f\"Seed prompt score: {seed_score:.4f}\")\n",
    "        \n",
    "        best_candidate = current_candidate.copy()\n",
    "        best_score = seed_score\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Iteration {iteration + 1}/{max_iterations}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            component_idx = iteration % len(components_to_optimize)\n",
    "            component = components_to_optimize[component_idx]\n",
    "            print(f\"Optimizing component: {component}\")\n",
    "            \n",
    "            minibatch = self._sample_minibatch(minibatch_size)\n",
    "            eval_result = self.adapter.evaluate(minibatch, current_candidate, capture_traces=True)\n",
    "            minibatch_score = sum(eval_result.scores) / len(eval_result.scores)\n",
    "            print(f\"Minibatch score: {minibatch_score:.4f}\")\n",
    "            \n",
    "            reflective_dataset = self.adapter.make_reflective_dataset(\n",
    "                current_candidate, eval_result, [component]\n",
    "            )\n",
    "            \n",
    "            print(\"Generating improved prompt via reflection...\")\n",
    "            new_prompt = self._generate_reflection(\n",
    "                current_candidate[component],\n",
    "                list(reflective_dataset.get(component, [])),\n",
    "                component\n",
    "            )\n",
    "            \n",
    "            new_candidate = current_candidate.copy()\n",
    "            new_candidate[component] = new_prompt\n",
    "            \n",
    "            print(f\"\\nProposed new {component}:\\n{new_prompt[:300]}...\")\n",
    "            \n",
    "            new_eval = self.adapter.evaluate(minibatch, new_candidate, capture_traces=False)\n",
    "            new_minibatch_score = sum(new_eval.scores) / len(new_eval.scores)\n",
    "            print(f\"New minibatch score: {new_minibatch_score:.4f}\")\n",
    "            \n",
    "            if new_minibatch_score > minibatch_score:\n",
    "                print(\"✓ Minibatch improved - evaluating on full valset...\")\n",
    "                new_val_score = self._evaluate_on_valset(new_candidate)\n",
    "                print(f\"Validation score: {new_val_score:.4f} (was {best_score:.4f})\")\n",
    "                \n",
    "                self.candidates.append(new_candidate.copy())\n",
    "                self.scores.append(new_val_score)\n",
    "                current_candidate = new_candidate.copy()\n",
    "                \n",
    "                if new_val_score > best_score:\n",
    "                    best_score = new_val_score\n",
    "                    best_candidate = new_candidate.copy()\n",
    "                    print(f\"★ New best! Score: {best_score:.4f}\")\n",
    "            else:\n",
    "                print(\"✗ No improvement on minibatch - keeping current candidate\")\n",
    "            \n",
    "            self.history.append({\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"component\": component,\n",
    "                \"minibatch_score_before\": minibatch_score,\n",
    "                \"minibatch_score_after\": new_minibatch_score,\n",
    "                \"accepted\": new_minibatch_score > minibatch_score,\n",
    "                \"best_val_score\": best_score\n",
    "            })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Optimization complete!\")\n",
    "        print(f\"Best validation score: {best_score:.4f} (started at {seed_score:.4f})\")\n",
    "        print(f\"Improvement: {(best_score - seed_score):.4f} ({(best_score - seed_score) / max(seed_score, 0.001) * 100:.1f}%)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return best_candidate\n",
    "\n",
    "print(\"Optimizer class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run optimizer\n",
    "print(\"5. Setting up optimizer...\")\n",
    "optimizer = SimpleGEPAOptimizer(\n",
    "    adapter=adapter,\n",
    "    trainset=trainset,\n",
    "    valset=valset,\n",
    "    reflection_model=refiner_model,\n",
    "    reflection_tokenizer=refiner_tokenizer,\n",
    "    device=CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "print(\"\\n6. Running GEPA optimization...\")\n",
    "best_prompts = optimizer.optimize(\n",
    "    seed_candidate=SEED_PROMPTS,\n",
    "    max_iterations=CONFIG[\"max_iterations\"],\n",
    "    minibatch_size=CONFIG[\"minibatch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "print(f\"\\n7. Saving results to {CONFIG['output_file']}...\")\n",
    "\n",
    "results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": CONFIG,\n",
    "    \"seed_prompts\": SEED_PROMPTS,\n",
    "    \"evolved_prompts\": best_prompts,\n",
    "    \"history\": optimizer.history,\n",
    "    \"final_scores\": {\n",
    "        \"seed\": optimizer.scores[0] if optimizer.scores else 0,\n",
    "        \"best\": max(optimizer.scores) if optimizer.scores else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(CONFIG[\"output_file\"], 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Done! Results saved to {CONFIG['output_file']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evolved prompts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVOLVED PROMPTS\")\n",
    "print(\"=\"*60)\n",
    "for name, prompt in best_prompts.items():\n",
    "    print(f\"\\n### {name}:\\n{prompt}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = [h['iteration'] for h in optimizer.history]\n",
    "best_scores = [h['best_val_score'] for h in optimizer.history]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, best_scores, 'b-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Best Validation Score')\n",
    "plt.title('GEPA Optimization Progress')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
